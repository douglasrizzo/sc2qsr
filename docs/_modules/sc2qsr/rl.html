<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>sc2qsr.rl</title>
    <link rel="stylesheet" href="../../_static/bootstrap-sphinx.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/graphviz.css" />
    <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../_static/doctools.js"></script>
    <script type="text/javascript" src="../../_static/language_data.js"></script>
    <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
<meta charset='utf-8'>
<meta http-equiv='X-UA-Compatible' content='IE=edge,chrome=1'>
<meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1'>
<meta name="apple-mobile-web-app-capable" content="yes">
<script type="text/javascript" src="../../_static/js/jquery-1.11.0.min.js "></script>
<script type="text/javascript" src="../../_static/js/jquery-fix.js "></script>
<script type="text/javascript" src="../../_static/bootstrap-3.3.7/js/bootstrap.min.js "></script>
<script type="text/javascript" src="../../_static/bootstrap-sphinx.js "></script>

  </head><body>

  <div id="navbar" class="navbar navbar-default navbar-fixed-top">
    <div class="container">
      <div class="navbar-header">
        <!-- .btn-navbar is used as the toggle for collapsed navbar content -->
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".nav-collapse">
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="../../index.html">
          sc2qsr</a>
        <span class="navbar-text navbar-version pull-left"><b>0.0.1</b></span>
      </div>

        <div class="collapse navbar-collapse nav-collapse">
          <ul class="nav navbar-nav">
            
            
              <li class="dropdown globaltoc-container">
  <a role="button"
     id="dLabelGlobalToc"
     data-toggle="dropdown"
     data-target="#"
     href="../../index.html">Site <b class="caret"></b></a>
  <ul class="dropdown-menu globaltoc"
      role="menu"
      aria-labelledby="dLabelGlobalToc"><ul>
<li class="toctree-l1"><a class="reference internal" href="../../agents.html">Agents – <code class="xref py py-mod docutils literal notranslate"><span class="pre">sc2qsr.agents</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../qualitative.html">Qualitative Spatial Reasoning – <code class="xref py py-mod docutils literal notranslate"><span class="pre">sc2qsr.spatial.qualitative</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../quantitative.html">Quantitative Spatial Functions – <code class="xref py py-mod docutils literal notranslate"><span class="pre">sc2qsr.spatial.quantitative</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../rl.html">Tabular reinforcement learning algorithms – <code class="xref py py-mod docutils literal notranslate"><span class="pre">sc2qsr.rl.tabular</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../unitstats.html">Unit Statistics from Liquipedia – <code class="xref py py-mod docutils literal notranslate"><span class="pre">sc2qsr.sc2info.unitstats</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../mapinfo.html">SC2 Map Information – <code class="xref py py-mod docutils literal notranslate"><span class="pre">sc2qsr.sc2info.mapinfo</span></code></a></li>
</ul>
</ul>
</li>
              
                <li class="dropdown">
  <a role="button"
     id="dLabelLocalToc"
     data-toggle="dropdown"
     data-target="#"
     href="#">Page <b class="caret"></b></a>
  <ul class="dropdown-menu localtoc"
      role="menu"
      aria-labelledby="dLabelLocalToc"></ul>
</li>
              
            
            
              
                
              
            
            
            
            
              <li class="hidden-sm"></li>
            
          </ul>

          
            
<form class="navbar-form navbar-right" action="../../search.html" method="get">
 <div class="form-group">
  <input type="text" name="q" class="form-control" placeholder="Search" />
 </div>
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
</form>
          
        </div>
    </div>
  </div>

<div class="container">
  <div class="row">
    <div class="body col-md-12 content" role="main">
      
  <h1>Source code for sc2qsr.rl</h1><div class="highlight"><pre>
<span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Tabular reinforcement learning algorithms -- :mod:`sc2qsr.rl.tabular`</span>
<span class="sd">======================================================================</span>

<span class="sd">https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="c1"># MIT License</span>

<span class="c1"># Copyright (c) 2017</span>

<span class="c1"># Permission is hereby granted, free of charge, to any person obtaining a copy</span>
<span class="c1"># of this software and associated documentation files (the &quot;Software&quot;), to deal</span>
<span class="c1"># in the Software without restriction, including without limitation the rights</span>
<span class="c1"># to use, copy, modify, merge, publish, distribute, sublicense, and/or sell</span>
<span class="c1"># copies of the Software, and to permit persons to whom the Software is</span>
<span class="c1"># furnished to do so, subject to the following conditions:</span>

<span class="c1"># The above copyright notice and this permission notice shall be included in all</span>
<span class="c1"># copies or substantial portions of the Software.</span>

<span class="c1"># THE SOFTWARE IS PROVIDED &quot;AS IS&quot;, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR</span>
<span class="c1"># IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,</span>
<span class="c1"># FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE</span>
<span class="c1"># AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER</span>
<span class="c1"># LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,</span>
<span class="c1"># OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE</span>
<span class="c1"># SOFTWARE.</span>

<span class="kn">from</span> <span class="nn">bisect</span> <span class="k">import</span> <span class="n">bisect</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">scipy.special</span> <span class="k">import</span> <span class="n">softmax</span>
<span class="kn">from</span> <span class="nn">sortedcontainers</span> <span class="k">import</span> <span class="n">SortedDict</span>


<span class="c1"># Adapted from https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow</span>
<span class="c1"># The MIT license was taken from there</span>
<div class="viewcode-block" id="TabularRLAlgorithm"><a class="viewcode-back" href="../../rl.html#sc2qsr.rl.TabularRLAlgorithm">[docs]</a><span class="k">class</span> <span class="nc">TabularRLAlgorithm</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Superclass for all tabular reinforcement learning algorithms&quot;&quot;&quot;</span>

    <span class="n">AVAILABLE_POLICIES</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;egreedy&#39;</span><span class="p">,</span> <span class="s1">&#39;boltzmann&#39;</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">actions</span><span class="p">,</span> <span class="n">policy</span><span class="o">=</span><span class="s1">&#39;egreedy&#39;</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">reward_decay</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">0.1</span>
    <span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Implementation of a tabular RL algorithm in Python</span>

<span class="sd">        :param actions: number of actions</span>
<span class="sd">        :type actions: int</span>
<span class="sd">        :param policy: name of one of the available policies, defaults to &#39;egreedy&#39;</span>
<span class="sd">        :type policy: str, optional</span>
<span class="sd">        :param learning_rate: learning rate of the algorithm, defaults to 0.01</span>
<span class="sd">        :type learning_rate: float, optional</span>
<span class="sd">        :param reward_decay: reward decay, defaults to 0.9</span>
<span class="sd">        :type reward_decay: float, optional</span>
<span class="sd">        :param epsilon: probability of taking a random action in the e-greedy policy, defaults to 0.1</span>
<span class="sd">        :type epsilon: float, optional</span>
<span class="sd">        :raises ValueError: if an unknown policy name is passed as argument</span>
<span class="sd">        :return: an object which implements functions to update the Q-table, as well as select actions according to policies and the values in the Q-table</span>
<span class="sd">        :rtype: QLearningTable</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">policy</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">TabularRLAlgorithm</span><span class="o">.</span><span class="n">AVAILABLE_POLICIES</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s1">&#39;Unknown policy </span><span class="se">\&#39;</span><span class="si">{}</span><span class="se">\&#39;</span><span class="s1">. Choose one from </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                    <span class="n">policy</span><span class="p">,</span> <span class="n">TabularRLAlgorithm</span><span class="o">.</span><span class="n">AVAILABLE_POLICIES</span>
                <span class="p">)</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">actions</span> <span class="o">=</span> <span class="n">actions</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">policy</span> <span class="o">=</span> <span class="n">policy</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">=</span> <span class="n">learning_rate</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">reward_decay</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="n">epsilon</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">q_table</span> <span class="o">=</span> <span class="n">SortedDict</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_boltzmann_policy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Select an action for the given state `s` according to a Boltzmann policy, in which the probabilities of each action being chosen is equal to their softmaxed values</span>

<span class="sd">        :param s: a state</span>
<span class="sd">        :type s: str</span>
<span class="sd">        :return: chosen action</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># https://stackoverflow.com/a/4442687/1245214</span>
        <span class="c1"># create a cdf of the softmaxed values and find where a</span>
        <span class="c1"># number between 0 and 1 would be inserted in the cdf list</span>
        <span class="n">softmaxed_q_values_cdf</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">q_table</span><span class="p">[</span><span class="n">s</span><span class="p">])</span><span class="o">.</span><span class="n">cumsum</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">bisect</span><span class="p">(</span><span class="n">softmaxed_q_values_cdf</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">())</span>

    <span class="k">def</span> <span class="nf">_egreedy_policy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Randomly select an action with probability epsilon, or select the best action for the given state `s` with probability 1 - epsilon</span>

<span class="sd">        :param s: a state</span>
<span class="sd">        :type s: str</span>
<span class="sd">        :return: chosen action</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">()</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span><span class="p">:</span>
            <span class="n">state_actions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_table</span><span class="p">[</span><span class="n">s</span><span class="p">]</span>

            <span class="c1"># get actions with largest value</span>
            <span class="n">best_actions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nonzero</span><span class="p">(</span><span class="n">state_actions</span> <span class="o">==</span> <span class="n">state_actions</span><span class="o">.</span><span class="n">max</span><span class="p">())[</span><span class="mi">0</span><span class="p">]</span>

            <span class="c1"># some actions may have the same value</span>
            <span class="n">action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">best_actions</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># choose random action</span>
            <span class="n">action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">actions</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">action</span>

    <span class="k">def</span> <span class="nf">choose_action</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_state_exist</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">policy</span> <span class="o">==</span> <span class="s1">&#39;egreedy&#39;</span><span class="p">:</span>
            <span class="n">action</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_egreedy_policy</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">policy</span> <span class="o">==</span> <span class="s1">&#39;boltzmann&#39;</span><span class="p">:</span>
            <span class="n">action</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_boltzmann_policy</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">action</span>

    <span class="k">def</span> <span class="nf">_check_state_exist</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">s</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_table</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">q_table</span><span class="p">[</span><span class="n">s</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">actions</span><span class="p">)</span>

<div class="viewcode-block" id="TabularRLAlgorithm.learn"><a class="viewcode-back" href="../../rl.html#sc2qsr.rl.TabularRLAlgorithm.learn">[docs]</a>    <span class="k">def</span> <span class="nf">learn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">a</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">r</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">s_</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Update the Q-table</span>

<span class="sd">        :param s: current state</span>
<span class="sd">        :param a: action taken</span>
<span class="sd">        :param r: reward signal</span>
<span class="sd">        :param s_: observed future state</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">pass</span></div></div>


<div class="viewcode-block" id="QLearning"><a class="viewcode-back" href="../../rl.html#sc2qsr.rl.QLearning">[docs]</a><span class="k">class</span> <span class="nc">QLearning</span><span class="p">(</span><span class="n">TabularRLAlgorithm</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Tabular Q-Learning algorithm&quot;&quot;&quot;</span>

<div class="viewcode-block" id="QLearning.learn"><a class="viewcode-back" href="../../rl.html#sc2qsr.rl.QLearning.learn">[docs]</a>    <span class="k">def</span> <span class="nf">learn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">a</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">r</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">s_</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_state_exist</span><span class="p">(</span><span class="n">s_</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_state_exist</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>

        <span class="n">q_predict</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_table</span><span class="p">[</span><span class="n">s</span><span class="p">][</span><span class="n">a</span><span class="p">]</span>
        <span class="n">q_target</span> <span class="o">=</span> <span class="n">r</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">q_table</span><span class="p">[</span><span class="n">s_</span><span class="p">])</span>

        <span class="c1"># update</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">q_table</span><span class="p">[</span><span class="n">s</span><span class="p">][</span><span class="n">a</span><span class="p">]</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">*</span> <span class="p">(</span><span class="n">q_target</span> <span class="o">-</span> <span class="n">q_predict</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="Sarsa"><a class="viewcode-back" href="../../rl.html#sc2qsr.rl.Sarsa">[docs]</a><span class="k">class</span> <span class="nc">Sarsa</span><span class="p">(</span><span class="n">TabularRLAlgorithm</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;SARSA algorithm&quot;&quot;&quot;</span>

<div class="viewcode-block" id="Sarsa.learn"><a class="viewcode-back" href="../../rl.html#sc2qsr.rl.Sarsa.learn">[docs]</a>    <span class="k">def</span> <span class="nf">learn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">a</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">r</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">s_</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_state_exist</span><span class="p">(</span><span class="n">s_</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_state_exist</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>

        <span class="n">q_predict</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_table</span><span class="p">[</span><span class="n">s</span><span class="p">][</span><span class="n">a</span><span class="p">]</span>
        <span class="n">q_target</span> <span class="o">=</span> <span class="n">r</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_table</span><span class="p">[</span><span class="n">s_</span><span class="p">][</span><span class="n">a</span><span class="p">]</span>

        <span class="c1"># update</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">q_table</span><span class="p">[</span><span class="n">s</span><span class="p">][</span><span class="n">a</span><span class="p">]</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">*</span> <span class="p">(</span><span class="n">q_target</span> <span class="o">-</span> <span class="n">q_predict</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="TabularRLLambdaAlgorithm"><a class="viewcode-back" href="../../rl.html#sc2qsr.rl.TabularRLLambdaAlgorithm">[docs]</a><span class="k">class</span> <span class="nc">TabularRLLambdaAlgorithm</span><span class="p">(</span><span class="n">TabularRLAlgorithm</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">actions</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">reward_decay</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">trace_decay</span><span class="o">=</span><span class="mf">0.9</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Superclass of tabular reinforcement learning algorithms with eligibility traces</span>

<span class="sd">        :param actions: number of actions</span>
<span class="sd">        :type actions: int</span>
<span class="sd">        :param policy: name of one of the available policies, defaults to &#39;egreedy&#39;</span>
<span class="sd">        :type policy: str, optional</span>
<span class="sd">        :param learning_rate: learning rate of the algorithm, defaults to 0.01</span>
<span class="sd">        :type learning_rate: float, optional</span>
<span class="sd">        :param reward_decay: reward decay, defaults to 0.9</span>
<span class="sd">        :type reward_decay: float, optional</span>
<span class="sd">        :param epsilon: probability of taking a random action in the e-greedy policy, defaults to 0.1</span>
<span class="sd">        :type epsilon: float, optional</span>
<span class="sd">        :param trace_decay: decay value for eligibility traces</span>
<span class="sd">        :type trace_decay: float, optional</span>
<span class="sd">        :raises ValueError: if an unknown policy name is passed as argument</span>
<span class="sd">        :return: an object which implements functions to update the Q-table, as well as select actions according to policies and the values in the Q-table</span>
<span class="sd">        :rtype: QLearningTable&quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">actions</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">,</span> <span class="n">reward_decay</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">)</span>

        <span class="c1"># backward view, eligibility trace.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lambda_</span> <span class="o">=</span> <span class="n">trace_decay</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eligibility_trace</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_table</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_check_state_exist</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">s</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_table</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">q_table</span><span class="p">[</span><span class="n">s</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">actions</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">eligibility_trace</span><span class="p">[</span><span class="n">s</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">actions</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">update_trace</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">error</span><span class="p">):</span>
        <span class="c1"># Method 1:</span>
        <span class="c1"># self.eligibility_trace[s][a] += 1</span>

        <span class="c1"># Method 2:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eligibility_trace</span><span class="p">[</span><span class="n">s</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">actions</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eligibility_trace</span><span class="p">[</span><span class="n">s</span><span class="p">][</span><span class="n">a</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>

        <span class="c1"># Q update</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">q_table</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">*</span> <span class="n">error</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">eligibility_trace</span>

        <span class="c1"># decay eligibility trace after update</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eligibility_trace</span> <span class="o">*=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">lambda_</span></div>


<div class="viewcode-block" id="QLambda"><a class="viewcode-back" href="../../rl.html#sc2qsr.rl.QLambda">[docs]</a><span class="k">class</span> <span class="nc">QLambda</span><span class="p">(</span><span class="n">TabularRLLambdaAlgorithm</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Q(λ) algorithm&quot;&quot;&quot;</span>

<div class="viewcode-block" id="QLambda.learn"><a class="viewcode-back" href="../../rl.html#sc2qsr.rl.QLambda.learn">[docs]</a>    <span class="k">def</span> <span class="nf">learn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">s_</span><span class="p">,</span> <span class="n">a_</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_state_exist</span><span class="p">(</span><span class="n">s_</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_state_exist</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
        <span class="n">q_predict</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_table</span><span class="p">[</span><span class="n">s</span><span class="p">][</span><span class="n">a</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">s_</span> <span class="o">!=</span> <span class="s1">&#39;terminal&#39;</span><span class="p">:</span>
            <span class="n">q_target</span> <span class="o">=</span> <span class="n">r</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">q_table</span><span class="p">[</span><span class="n">s_</span><span class="p">])</span>  <span class="c1"># next state is not terminal</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">q_target</span> <span class="o">=</span> <span class="n">r</span>  <span class="c1"># next state is terminal</span>
        <span class="n">error</span> <span class="o">=</span> <span class="n">q_target</span> <span class="o">-</span> <span class="n">q_predict</span>

        <span class="c1"># increase trace amount for visited state-action pair</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">update_trace</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">error</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="SarsaLambda"><a class="viewcode-back" href="../../rl.html#sc2qsr.rl.SarsaLambda">[docs]</a><span class="k">class</span> <span class="nc">SarsaLambda</span><span class="p">(</span><span class="n">TabularRLLambdaAlgorithm</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Sarsa(λ) algorithm&quot;&quot;&quot;</span>

<div class="viewcode-block" id="SarsaLambda.learn"><a class="viewcode-back" href="../../rl.html#sc2qsr.rl.SarsaLambda.learn">[docs]</a>    <span class="k">def</span> <span class="nf">learn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">s_</span><span class="p">,</span> <span class="n">a_</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_state_exist</span><span class="p">(</span><span class="n">s_</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_state_exist</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
        <span class="n">q_predict</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_table</span><span class="p">[</span><span class="n">s</span><span class="p">][</span><span class="n">a</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">s_</span> <span class="o">!=</span> <span class="s1">&#39;terminal&#39;</span><span class="p">:</span>
            <span class="n">q_target</span> <span class="o">=</span> <span class="n">r</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_table</span><span class="p">[</span><span class="n">s_</span><span class="p">][</span><span class="n">a_</span><span class="p">]</span>  <span class="c1"># next state is not terminal</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">q_target</span> <span class="o">=</span> <span class="n">r</span>  <span class="c1"># next state is terminal</span>
        <span class="n">error</span> <span class="o">=</span> <span class="n">q_target</span> <span class="o">-</span> <span class="n">q_predict</span>

        <span class="c1"># increase trace amount for visited state-action pair</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">update_trace</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">error</span><span class="p">)</span></div></div>
</pre></div>

    </div>
      
  </div>
</div>
<footer class="footer">
  <div class="container">
    <p class="pull-right">
      <a href="#">Back to top</a>
      
    </p>
    <p>
        &copy; Copyright 2019, Douglas De Rizzo Meneghetti.<br/>
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 2.3.1.<br/>
    </p>
  </div>
</footer>
  </body>
</html>